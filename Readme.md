# RAG + MCP (Ollama) — Transport Policy Demo

This repo shows how to connect a **local RAG (Retrieval-Augmented Generation) service** on top of **Ollama** to an **MCP server** that exposes a single tool, `getTransportPolicy`, to MCP‑enabled clients (e.g. Claude Desktop).

The knowledge base is a real policy document: **Pakistan’s Transport Policy** (PDF). The goal is to demonstrate how you can turn a local PDF into a queryable tool, answering natural‑language questions grounded in that policy.

---

## What this project does (and why)

- **Idea / motive**  
  Make it easy to experiment with _local_, _policy‑aware_ assistants where:

  - Data never leaves your machine.
  - The model is forced to answer based on a specific source (here, Pakistan’s Transport Policy PDF).

- **What it actually does**

  - Loads the **Pakistan’s Transport Policy** PDF.
  - Chunks and embeds the text into an in‑memory vector store.
  - Uses an Ollama‑hosted model to answer questions with relevant policy context.
  - Wraps this RAG pipeline behind an MCP tool, `getTransportPolicy`, so any MCP client can call it.

- **How it works (high level)**
  1. MCP client calls `getTransportPolicy` with a `query` string.
  2. MCP server forwards the query to the local RAG HTTP service.
  3. RAG service retrieves relevant chunks from **Pakistan’s Transport Policy** and sends them to the LLM.
  4. The answer is returned to the MCP client as both human‑readable `content` and structured `{ answer: string }`.

---

## Architecture

- **RAG Service (`rag-service/`)**

  - Node.js + Express HTTP API.
  - Uses LangChain + Ollama for:
    - PDF loading and chunking.
    - Embeddings (`nomic-embed-text`) + in‑memory vector store.
    - Question‑answering via `qwen2.5:1.5b`.
  - Exposes `POST /query` on **http://localhost:3001/query** returning `{ answer: string }`.

- **MCP Server (`mcp-server/`)**

  - Built with `@modelcontextprotocol/sdk`.
  - Registers **`getTransportPolicy`**.
  - On each call:
    - Sends the `query` to the RAG service.
    - Returns both display `content` and `structuredContent: { answer }`.

- **MCP Client (e.g. Claude Desktop)**
  - Connects to the MCP server over stdio.
  - Exposes the `getTransportPolicy` tool so you can ask questions about **Pakistan’s Transport Policy** directly from the chat UI.

---

## Tools

- **`getTransportPolicy`**
  - **Input:**
    - `query` (string) — Your transport‑policy question.
  - **Output (`structuredContent`):**
    - `answer` (string) — Answer grounded in Pakistan’s Transport Policy PDF.
  - **Behavior:**
    - Forwards `query` to `http://localhost:3001/query`.
    - RAG service performs retrieval over the PDF and uses the LLM to generate an answer.

---

## Prerequisites

- **General**

  - Node.js **18+** and npm
  - Git (optional)

- **Ollama**

  - Install: https://ollama.com
  - Ensure the Ollama daemon is running (`ollama serve`).

- **Models used**

```bash
ollama pull qwen2.5:1.5b
ollama pull nomic-embed-text
```

---

## Project Layout

- **`rag-service/`** – RAG HTTP API exposing `/query`.
- **`mcp-server/`** – MCP server exposing `getTransportPolicy`.
- **`rag-service/assets/transport_policy.pdf`** – **Pakistan’s Transport Policy** PDF used as the sole knowledge source.

---

## Setup

From the project root:

```bash
git clone <your-repo-url> rag-mcp-ollama
cd rag-mcp-ollama
```

- **RAG service**

```bash
cd rag-service
npm install
```

- **MCP server**

```bash
cd ../mcp-server
npm install
npm run build
```

---

## Run locally

You need **two** processes: the RAG HTTP service and the MCP server.

1. **Start Ollama** (if needed):

```bash
ollama serve
ollama pull qwen2.5:1.5b
ollama pull nomic-embed-text
```

2. **Start the RAG service**:

```bash
cd rag-mcp-ollama/rag-service
npm start
```

This will:

- Load `assets/transport_policy.pdf` (Pakistan’s Transport Policy).
- Chunk text and build a `MemoryVectorStore` with `nomic-embed-text` embeddings.
- Initialize `qwen2.5:1.5b`.
- Start an Express server at `http://localhost:3001` with `POST /query`.

3. **Start the MCP server**:

```bash
cd rag-mcp-ollama/mcp-server
npm run build   # if not built yet
node build/index.js
```

The server logs `Transport MCP Server running on stdio` and exposes `getTransportPolicy`.

---

## Using with Claude Desktop (example)

Configure an MCP server entry that runs the Node process, for example:

```jsonc
{
  "transport-mcp": {
    "command": "node",
    "args": ["/absolute/path/to/rag-mcp-ollama/mcp-server/build/index.js"],
    "env": {}
  }
}
```

Once enabled, you should see a **`transport-mcp`** server with the **`getTransportPolicy`** tool. Ask questions like:

- "What are the main objectives of Pakistan’s Transport Policy?"
- "How does the policy address road safety?"

Answers will be generated by the local RAG pipeline over the Pakistan’s Transport Policy PDF.

---

## Troubleshooting

- **RAG service not responding**

  - Ensure `npm start` is running in `rag-service/` on port 3001.
  - Verify Ollama is running and both models are pulled.

- **MCP client schema / validation errors**

  - Ensure you are running the built server (`node build/index.js`).

- **Model errors or slow responses**
  - Check `ollama serve` is healthy and your system has enough resources.

---

## License

This project is provided as an example of building a local, PDF‑grounded RAG + MCP integration with Ollama. Adjust the license text as needed for your use case.
